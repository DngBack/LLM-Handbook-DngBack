# LLM-Notes 🚀

A comprehensive collection of notes, insights, and experiments on Large Language Models (LLMs). This repository serves as a knowledge hub for everything related to LLMs, from theoretical foundations to practical deployment, fine-tuning strategies, and experimental implementations.

## Table Of Contents

- [📌 Introduction to LLMs](#introduction-to-llms): Basics, architectures, mathematical foundations, and key concepts.
- [🛠 LLM Fine-tuning with SFT](#llm-fine-tuning-with-sft): Techniques, frameworks, and best practices.
- [🎮 Deployment Strategies](#deployment-strategies): Serving LLMs using ONNX, FastAPI, Triton, and other tools.
- [💪 Optimization & Scaling](#optimization--scaling): Quantization, pruning, and performance tuning.
- **RLHF & Customization** – Reinforcement learning and aligning models for specific use cases.
- [🔄 RLHF & Customization](#rlhf--customization): Case studies, automation, and integration examples.
- [🌐 Real-world Applications](#real-world-applications): Scripts and test cases to evaluate different models and settings.

## 🚀 Why This Repo?

- Deeply explores the theory and mathematics behind LLMs.
- Provides practical test repositories to evaluate models.
- Offers a hands-on approach with experiments and deployment guides.
- Serves as a resource hub for engineers, researchers, and enthusiasts.

## 🛠 Tech Stack & Tools

- **Frameworks**: PyTorch, TensorFlow, ONNX
- **Serving**: FastAPI, Triton Inference Server
- **Optimization**: Quantization, LoRA, GPTQ
- **RLHF**: Reinforcement learning techniques for fine-tuning

## 🧪 Ideas for Testing & Running LLMs

- **Benchmark different LLMs** on speed, accuracy, and memory usage.
- **Experiment with fine-tuning** using LoRA and full model updates.
- **Compare quantization methods** (GPTQ, AWQ, INT8, FP16, etc.).
- **Deploy LLMs with different serving frameworks** (ONNX, vLLM, Triton).
- **Test RLHF methods** to align models better with specific tasks.
- **Run adversarial tests** to check model robustness.
- **Measure token efficiency** to optimize inference costs.

# Quick Links
